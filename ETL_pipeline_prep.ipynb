{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ETL_pipeline_prep.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+A4naiX4Y5mYkDFujhZMy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IprFR0eTSesl"},"source":["### **Import libraries and load datasets.**\r\n","\r\n","* Import Python libraries\r\n","* Load messages.csv into a dataframe and inspect the first few lines.\r\n","* Load categories.csv into a dataframe and inspect the first few lines."]},{"cell_type":"code","metadata":{"id":"yVWcPxPKSX2u","executionInfo":{"status":"ok","timestamp":1616001279483,"user_tz":-120,"elapsed":717,"user":{"displayName":"Andreas Stavrou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjGF1_4uyTKqzXUltL6yicZH1zqqI44xErXQFnuw=s64","userId":"03248271435297869716"}}},"source":["# import libraries\r\n","import pandas as pd\r\n","from sqlalchemy import create_engine"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiaT5FnHSciw"},"source":["# load messages dataset\r\n","messages = pd.read_csv('messages.csv')\r\n","messages.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YijE9lllSoRv"},"source":["# load categories dataset\r\n","categories = pd.read_csv('categories.csv')\r\n","categories.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WH9VPZvgSrJK"},"source":["### **Merge datasets.**\r\n","\r\n","* Merge the messages and categories datasets using the common id\r\n","* Assign this combined dataset to df, which will be cleaned in the following steps"]},{"cell_type":"code","metadata":{"id":"KNrwTf89SoPa"},"source":["# merge datasets\r\n","df = messages.merge(categories, how='outer',on=['id'])\r\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bulUDvKWSzbV"},"source":["### **Split categories into separate category columns.**\r\n","\r\n","* Split the values in the categories column on the ; character so that each value becomes a separate column. You'll find this method very helpful! Make sure to set expand=True.\r\n","* Use the first row of categories dataframe to create column names for the categories data.\r\n","* Rename columns of categories with new column names."]},{"cell_type":"code","metadata":{"id":"g2XS1j_TSoNT"},"source":["detail_categories = categories.categories.str.split(';', expand=True)\r\n","\r\n","new_header = detail_categories.iloc[0].str.split('-').str.get(0) #grab the first row for the header\r\n","#detail_categories = detail_categories[1:] #take the data less the header row\r\n","detail_categories.columns = new_header #set the header row as the df header\r\n","\r\n","for column in detail_categories:\r\n","    # set each value to be the last character of the string\r\n","    detail_categories[column] = detail_categories[column].str.split('-').str.get(1)\r\n","    \r\n","    # convert column from string to numeric\r\n","    detail_categories[column] = detail_categories[column].astype(int)\r\n","\r\n","detail_categories"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ychiJpTeSoK1"},"source":["# drop the original categories column from `df`\r\n","df.drop(['categories'], axis=1, inplace=True)\r\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxZkHbkpSoI3"},"source":["# concatenate the original dataframe with the new `categories` dataframe\r\n","df = pd.concat([df, detail_categories], axis=1, join='inner')\r\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzYhuXWRTC01"},"source":["### **Remove duplicates.**\r\n","\r\n","* Check how many duplicates are in this dataset.\r\n","* Drop the duplicates.\r\n","* Confirm duplicates were removed."]},{"cell_type":"code","metadata":{"id":"EmIs5KGNSoGZ"},"source":["cols=['id', 'message', 'original', 'genre', 'related', 'request', 'offer',\r\n","       'aid_related', 'medical_help', 'medical_products', 'search_and_rescue',\r\n","       'security', 'military', 'child_alone', 'water', 'food', 'shelter',\r\n","       'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid',\r\n","       'infrastructure_related', 'transport', 'buildings', 'electricity',\r\n","       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\r\n","       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\r\n","       'other_weather', 'direct_report', 'related', 'request', 'offer',\r\n","       'aid_related', 'medical_help', 'medical_products', 'search_and_rescue',\r\n","       'security', 'military', 'child_alone', 'water', 'food', 'shelter',\r\n","       'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid',\r\n","       'infrastructure_related', 'transport', 'buildings', 'electricity',\r\n","       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\r\n","       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\r\n","       'other_weather', 'direct_report']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGDorgGmSoD8"},"source":["# check number of duplicates \r\n","df.duplicated(subset=cols, keep='first').sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsFlaC1tTL4G"},"source":["# drop duplicates\r\n","df.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tQ1KiwnTL2B"},"source":["# check number of duplicates\r\n","# 26386 rows before and 26345 after so 41 dups, looks, good"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avUX4_mBTQKl"},"source":["### **Save the clean dataset into an sqlite database.**\r\n","\r\n","You can do this with pandas to_sql method combined with the SQLAlchemy library. Remember to import SQLAlchemy's create_engine in the first cell of this notebook to use it below."]},{"cell_type":"code","metadata":{"id":"yt83iuovTLzZ"},"source":["engine = create_engine('sqlite:///InsertDatabaseName.db')\r\n","df.to_sql('Messages', engine, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_kJlfevFTYH_"},"source":["### **Use this notebook to complete etl_pipeline.py**"]},{"cell_type":"code","metadata":{"id":"mscR9lHATZDv"},"source":[""],"execution_count":null,"outputs":[]}]}